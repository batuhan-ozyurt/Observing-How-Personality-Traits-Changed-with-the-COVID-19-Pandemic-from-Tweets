{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1ldCOWQ7pNQ"
   },
   "source": [
    "# Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "_tz82NeZ7pNU"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOeSmQg-7pNa"
   },
   "source": [
    "# Reading the Data and Splitting Them into Two Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbMlMyO07pNb"
   },
   "source": [
    "Create two sets: One is tweets tweeted before 30 November, 2019, the other is after 16 March 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ifWX9LM7pNb",
    "outputId": "cff538a5-5151-4ba9-98ca-4276cfe7a2c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,13,15,17,18,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "tweets_expanded = pd.read_csv(\"/content/drive/MyDrive/tweets_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLCU3zLp7pNb",
    "outputId": "71003e84-24d0-40c9-d5cd-cd62907b21ba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>province_codes</th>\n",
       "      <th>genders</th>\n",
       "      <th>following</th>\n",
       "      <th>...</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>downloaded</th>\n",
       "      <th>ref_twt_id_str</th>\n",
       "      <th>ref_twt_txt</th>\n",
       "      <th>ref_usr_id_str</th>\n",
       "      <th>type</th>\n",
       "      <th>twt_date</th>\n",
       "      <th>twt_id_str</th>\n",
       "      <th>twt_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>istanbul, türkiye</td>\n",
       "      <td>karınca yükünü fil çekmez oldu..</td>\n",
       "      <td>yılmaz şengül</td>\n",
       "      <td>deatime</td>\n",
       "      <td>31109031</td>\n",
       "      <td>Tue Apr 14 12:22:59 +0000 2009</td>\n",
       "      <td>[{'source': 'location', 'pcode': 34}]</td>\n",
       "      <td>[{'source': 'name', 'gender': 'male'}]</td>\n",
       "      <td>['1481278402514636806', '15144122', '104272079...</td>\n",
       "      <td>...</td>\n",
       "      <td>1851.0</td>\n",
       "      <td>1099.0</td>\n",
       "      <td>220318</td>\n",
       "      <td>1.481613e+18</td>\n",
       "      <td>Ekmek küçülüyor, hem kamuda hem özelde… \\n\\nTü...</td>\n",
       "      <td>1.481278e+18</td>\n",
       "      <td>retweet</td>\n",
       "      <td>220113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>istanbul, türkiye</td>\n",
       "      <td>karınca yükünü fil çekmez oldu..</td>\n",
       "      <td>yılmaz şengül</td>\n",
       "      <td>deatime</td>\n",
       "      <td>31109031</td>\n",
       "      <td>Tue Apr 14 12:22:59 +0000 2009</td>\n",
       "      <td>[{'source': 'location', 'pcode': 34}]</td>\n",
       "      <td>[{'source': 'name', 'gender': 'male'}]</td>\n",
       "      <td>['1481278402514636806', '15144122', '104272079...</td>\n",
       "      <td>...</td>\n",
       "      <td>1851.0</td>\n",
       "      <td>1099.0</td>\n",
       "      <td>220318</td>\n",
       "      <td>1.481568e+18</td>\n",
       "      <td>Enflasyon Oranında Iyileştirme istiyoruz .\\n\\n...</td>\n",
       "      <td>1.081091e+18</td>\n",
       "      <td>retweet</td>\n",
       "      <td>220113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>istanbul, türkiye</td>\n",
       "      <td>karınca yükünü fil çekmez oldu..</td>\n",
       "      <td>yılmaz şengül</td>\n",
       "      <td>deatime</td>\n",
       "      <td>31109031</td>\n",
       "      <td>Tue Apr 14 12:22:59 +0000 2009</td>\n",
       "      <td>[{'source': 'location', 'pcode': 34}]</td>\n",
       "      <td>[{'source': 'name', 'gender': 'male'}]</td>\n",
       "      <td>['1481278402514636806', '15144122', '104272079...</td>\n",
       "      <td>...</td>\n",
       "      <td>1851.0</td>\n",
       "      <td>1099.0</td>\n",
       "      <td>220318</td>\n",
       "      <td>1.481489e+18</td>\n",
       "      <td>Biz Taşeronuz , O Eşek KADROLU. \\n\\n450 Bin Be...</td>\n",
       "      <td>1.081091e+18</td>\n",
       "      <td>retweet</td>\n",
       "      <td>220113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>istanbul, türkiye</td>\n",
       "      <td>karınca yükünü fil çekmez oldu..</td>\n",
       "      <td>yılmaz şengül</td>\n",
       "      <td>deatime</td>\n",
       "      <td>31109031</td>\n",
       "      <td>Tue Apr 14 12:22:59 +0000 2009</td>\n",
       "      <td>[{'source': 'location', 'pcode': 34}]</td>\n",
       "      <td>[{'source': 'name', 'gender': 'male'}]</td>\n",
       "      <td>['1481278402514636806', '15144122', '104272079...</td>\n",
       "      <td>...</td>\n",
       "      <td>1851.0</td>\n",
       "      <td>1099.0</td>\n",
       "      <td>220318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>original</td>\n",
       "      <td>220112</td>\n",
       "      <td>1.481377e+18</td>\n",
       "      <td>Efsaneler Ölmez ..\\n\\n#Lefter https://t.co/bl9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>istanbul, türkiye</td>\n",
       "      <td>karınca yükünü fil çekmez oldu..</td>\n",
       "      <td>yılmaz şengül</td>\n",
       "      <td>deatime</td>\n",
       "      <td>31109031</td>\n",
       "      <td>Tue Apr 14 12:22:59 +0000 2009</td>\n",
       "      <td>[{'source': 'location', 'pcode': 34}]</td>\n",
       "      <td>[{'source': 'name', 'gender': 'male'}]</td>\n",
       "      <td>['1481278402514636806', '15144122', '104272079...</td>\n",
       "      <td>...</td>\n",
       "      <td>1851.0</td>\n",
       "      <td>1099.0</td>\n",
       "      <td>220318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>original</td>\n",
       "      <td>220112</td>\n",
       "      <td>1.481341e+18</td>\n",
       "      <td>HALK 'ın Ekmeğidir ADALET ..\\n\\nSiz Vermeyecek...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0           location                       description  \\\n",
       "0          0  istanbul, türkiye  karınca yükünü fil çekmez oldu..   \n",
       "1          1  istanbul, türkiye  karınca yükünü fil çekmez oldu..   \n",
       "2          2  istanbul, türkiye  karınca yükünü fil çekmez oldu..   \n",
       "3          3  istanbul, türkiye  karınca yükünü fil çekmez oldu..   \n",
       "4          4  istanbul, türkiye  karınca yükünü fil çekmez oldu..   \n",
       "\n",
       "            name screen_name    id_str                      created_at  \\\n",
       "0  yılmaz şengül     deatime  31109031  Tue Apr 14 12:22:59 +0000 2009   \n",
       "1  yılmaz şengül     deatime  31109031  Tue Apr 14 12:22:59 +0000 2009   \n",
       "2  yılmaz şengül     deatime  31109031  Tue Apr 14 12:22:59 +0000 2009   \n",
       "3  yılmaz şengül     deatime  31109031  Tue Apr 14 12:22:59 +0000 2009   \n",
       "4  yılmaz şengül     deatime  31109031  Tue Apr 14 12:22:59 +0000 2009   \n",
       "\n",
       "                          province_codes  \\\n",
       "0  [{'source': 'location', 'pcode': 34}]   \n",
       "1  [{'source': 'location', 'pcode': 34}]   \n",
       "2  [{'source': 'location', 'pcode': 34}]   \n",
       "3  [{'source': 'location', 'pcode': 34}]   \n",
       "4  [{'source': 'location', 'pcode': 34}]   \n",
       "\n",
       "                                  genders  \\\n",
       "0  [{'source': 'name', 'gender': 'male'}]   \n",
       "1  [{'source': 'name', 'gender': 'male'}]   \n",
       "2  [{'source': 'name', 'gender': 'male'}]   \n",
       "3  [{'source': 'name', 'gender': 'male'}]   \n",
       "4  [{'source': 'name', 'gender': 'male'}]   \n",
       "\n",
       "                                           following  ... followers_count  \\\n",
       "0  ['1481278402514636806', '15144122', '104272079...  ...          1851.0   \n",
       "1  ['1481278402514636806', '15144122', '104272079...  ...          1851.0   \n",
       "2  ['1481278402514636806', '15144122', '104272079...  ...          1851.0   \n",
       "3  ['1481278402514636806', '15144122', '104272079...  ...          1851.0   \n",
       "4  ['1481278402514636806', '15144122', '104272079...  ...          1851.0   \n",
       "\n",
       "   following_count  downloaded ref_twt_id_str  \\\n",
       "0           1099.0      220318   1.481613e+18   \n",
       "1           1099.0      220318   1.481568e+18   \n",
       "2           1099.0      220318   1.481489e+18   \n",
       "3           1099.0      220318            NaN   \n",
       "4           1099.0      220318            NaN   \n",
       "\n",
       "                                         ref_twt_txt ref_usr_id_str      type  \\\n",
       "0  Ekmek küçülüyor, hem kamuda hem özelde… \\n\\nTü...   1.481278e+18   retweet   \n",
       "1  Enflasyon Oranında Iyileştirme istiyoruz .\\n\\n...   1.081091e+18   retweet   \n",
       "2  Biz Taşeronuz , O Eşek KADROLU. \\n\\n450 Bin Be...   1.081091e+18   retweet   \n",
       "3                                                NaN            NaN  original   \n",
       "4                                                NaN            NaN  original   \n",
       "\n",
       "  twt_date    twt_id_str                                            twt_txt  \n",
       "0   220113           NaN                                                NaN  \n",
       "1   220113           NaN                                                NaN  \n",
       "2   220113           NaN                                                NaN  \n",
       "3   220112  1.481377e+18  Efsaneler Ölmez ..\\n\\n#Lefter https://t.co/bl9...  \n",
       "4   220112  1.481341e+18  HALK 'ın Ekmeğidir ADALET ..\\n\\nSiz Vermeyecek...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_expanded.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzIXp1In7pNc",
    "outputId": "a5cef88e-6185-43a7-ea77-c0d354d561e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'location', 'description', 'name', 'screen_name',\n",
       "       'id_str', 'created_at', 'province_codes', 'genders', 'following',\n",
       "       'followers', 'followers_count', 'following_count', 'downloaded',\n",
       "       'ref_twt_id_str', 'ref_twt_txt', 'ref_usr_id_str', 'type', 'twt_date',\n",
       "       'twt_id_str', 'twt_txt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_expanded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "OoiNnpdN7pNc"
   },
   "outputs": [],
   "source": [
    "tweets_expanded = tweets_expanded.drop(tweets_expanded.columns[[0,1,2,3,4, 5,6,7,8,9, 10,11,12,13, 14,16,19]], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVhfVF7X7pNd"
   },
   "source": [
    "Size of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "gvWcqJf07pNd"
   },
   "outputs": [],
   "source": [
    "len_pre = 10000\n",
    "len_pandemic = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "kyeHnd617pNd"
   },
   "outputs": [],
   "source": [
    "pre_covid = pd.DataFrame(columns=list(tweets_expanded.columns))\n",
    "pandemic = pd.DataFrame(columns=list(tweets_expanded.columns))\n",
    "\n",
    "for i, date in enumerate(tweets_expanded[\"twt_date\"]):\n",
    "    if int(date) < 191200 and len(pre_covid) < len_pre:\n",
    "        pre_covid = pre_covid.append(tweets_expanded.loc[i,:])\n",
    "    elif int(date) > 200315 and len(pandemic) < len_pandemic:\n",
    "        pandemic = pandemic.append(tweets_expanded.loc[i,:])\n",
    "    if (len(pre_covid) + len(pandemic)) == (len_pre + len_pandemic):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "C6z8FA3E7pNd",
    "outputId": "b9aa34c0-18f6-495c-8079-e779ff5ba3e6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1c4e4e7c-e97a-41bb-a39a-146ef9f33931\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref_twt_txt</th>\n",
       "      <th>type</th>\n",
       "      <th>twt_date</th>\n",
       "      <th>twt_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29508</th>\n",
       "      <td>Be any #colour you choose to be, be yourself, ...</td>\n",
       "      <td>fav</td>\n",
       "      <td>190606</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29621</th>\n",
       "      <td>allaha sukur gonlumuz genıs, ask insanıyız, at...</td>\n",
       "      <td>fav</td>\n",
       "      <td>190305</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29623</th>\n",
       "      <td>Empati yapmak başkasının yerine de düşünebilme...</td>\n",
       "      <td>fav</td>\n",
       "      <td>160103</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29691</th>\n",
       "      <td>NaN</td>\n",
       "      <td>original</td>\n",
       "      <td>170208</td>\n",
       "      <td>https://t.co/7cQMeuAir9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29692</th>\n",
       "      <td>NaN</td>\n",
       "      <td>reply</td>\n",
       "      <td>161208</td>\n",
       "      <td>@turk_videolar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c4e4e7c-e97a-41bb-a39a-146ef9f33931')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-1c4e4e7c-e97a-41bb-a39a-146ef9f33931 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-1c4e4e7c-e97a-41bb-a39a-146ef9f33931');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                             ref_twt_txt      type twt_date  \\\n",
       "29508  Be any #colour you choose to be, be yourself, ...       fav   190606   \n",
       "29621  allaha sukur gonlumuz genıs, ask insanıyız, at...       fav   190305   \n",
       "29623  Empati yapmak başkasının yerine de düşünebilme...       fav   160103   \n",
       "29691                                                NaN  original   170208   \n",
       "29692                                                NaN     reply   161208   \n",
       "\n",
       "                       twt_txt  \n",
       "29508                      NaN  \n",
       "29621                      NaN  \n",
       "29623                      NaN  \n",
       "29691  https://t.co/7cQMeuAir9  \n",
       "29692           @turk_videolar  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_covid.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwJRgIDm7pNe",
    "outputId": "81e48894-775a-419a-ee9c-ae7855a40fd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ref_twt_txt                        NaN\n",
       "type                          original\n",
       "twt_date                        190611\n",
       "twt_txt        https://t.co/ssEEDk8oPe\n",
       "Name: 247, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_expanded.loc[247,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "FEdJXcTE7pNe"
   },
   "outputs": [],
   "source": [
    "pre_covid_tweets = []\n",
    "pandemic_tweets = []\n",
    "\n",
    "for i, Bool in pre_covid.isnull()[\"twt_txt\"].items():\n",
    "    if Bool:\n",
    "        pre_covid_tweets.append(pre_covid.loc[i,\"ref_twt_txt\"])\n",
    "    else:\n",
    "        pre_covid_tweets.append(pre_covid.loc[i,\"twt_txt\"])\n",
    "\n",
    "for i, Bool in pandemic.isnull()[\"twt_txt\"].items():\n",
    "    if Bool:\n",
    "        pandemic_tweets.append(pandemic.loc[i,\"ref_twt_txt\"])\n",
    "    else:\n",
    "        pandemic_tweets.append(pandemic.loc[i,\"twt_txt\"])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "j52-JWWhi0Mm"
   },
   "outputs": [],
   "source": [
    "random.shuffle(pre_covid_tweets)\n",
    "random.shuffle(pandemic_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "3F4QQ1JljQ8Z"
   },
   "outputs": [],
   "source": [
    "pre_covid_tweets = pre_covid_tweets[:500]\n",
    "pandemic_tweets = pandemic_tweets[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-VczM367pNe"
   },
   "outputs": [],
   "source": [
    "pre_covid_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rw8dr8A87pNe"
   },
   "outputs": [],
   "source": [
    "pandemic_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0W8VqzT7pNf"
   },
   "source": [
    "# Sentence Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ZcGwUGMCTKp",
    "outputId": "12f3942b-118b-4b80-d98b-9b1f5cbbb937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.7.0)\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "IGvDD9Eb7pNf"
   },
   "outputs": [],
   "source": [
    "from emoji import UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "OAdfTnul7pNf"
   },
   "outputs": [],
   "source": [
    "separators = [\"\\n\", \".\", \":\", \"!\", \"?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "R0xLtpRx7pNf"
   },
   "outputs": [],
   "source": [
    "def no_link(tweet):    \n",
    "    ind1 = tweet.find(' ')\n",
    "    ind2 = tweet.find('\\n')\n",
    "    if ind1 == -1:\n",
    "        ind1 = 10000\n",
    "    if ind2 == -1:\n",
    "        ind2 = 10000\n",
    "    if ind1 < ind2:\n",
    "        tweet = tweet[ind1:]\n",
    "    elif ind1 > ind2:\n",
    "        tweet = tweet[ind2:]\n",
    "    else:\n",
    "        tweet = ''\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "up_mCstB7pNf"
   },
   "outputs": [],
   "source": [
    "pre_covid_sentences = []\n",
    "link_headers = ['http://t.co/','https://t.co/']\n",
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    tweets_wo_link = []\n",
    "    x = 0\n",
    "    y = 0    \n",
    "    if link_headers[0] in tweet:\n",
    "        x = 1\n",
    "    if link_headers[1] in tweet:\n",
    "        y = 1\n",
    "    while tweet:\n",
    "        if x == 0 and y == 0:\n",
    "            tweets_wo_link.append(tweet)\n",
    "            tweet = False\n",
    "        elif x == 0 and y == 1:\n",
    "            ind = tweet.find(link_headers[1]) \n",
    "            tweets_wo_link.append(tweet[0:ind])\n",
    "            tweet = tweet[ind:]\n",
    "            tweet = no_link(tweet)\n",
    "                \n",
    "            if link_headers[1] not in tweet:\n",
    "                y = 0\n",
    "                \n",
    "        elif x == 1 and y == 0:\n",
    "            ind = tweet.find(link_headers[0])\n",
    "            tweets_wo_link.append(tweet[0:ind])\n",
    "            tweet = tweet[ind:]\n",
    "            tweet = no_link(tweet)\n",
    "            if link_headers[0] not in tweet:\n",
    "                x = 0\n",
    "               \n",
    "        elif x == 1 and y == 1:\n",
    "            ind0 = tweet.find(link_headers[0])\n",
    "            ind1 = tweet.find(link_headers[1])\n",
    "            if ind0 < ind1:\n",
    "                ind = tweet.find(link_headers[0])\n",
    "                tweets_wo_link.append(tweet[0:ind])\n",
    "                tweet = tweet[ind:]\n",
    "                tweet = no_link(tweet)\n",
    "            elif ind0 > ind1:\n",
    "                ind = tweet.find(link_headers[1])\n",
    "                tweets_wo_link.append(tweet[0:ind])\n",
    "                tweet = tweet[ind:]\n",
    "                tweet = no_link(tweet)            \n",
    "            if link_headers[1] not in tweet:\n",
    "                y = 0\n",
    "            if link_headers[0] not in tweet:\n",
    "                x = 0\n",
    "    tweets_wo_link = [x for x in tweets_wo_link if len(x) != 0]\n",
    "    sentences = []\n",
    "    j = 0\n",
    "    for tweet_wo_link in tweets_wo_link:\n",
    "        for i, char in enumerate(tweet_wo_link):\n",
    "            if char in UNICODE_EMOJI['en'] or char in separators:\n",
    "                sentences.append(tweet_wo_link[j:i])\n",
    "                j = i + 1\n",
    "        sentences.append(tweet_wo_link[j:])\n",
    "    sentences = [x for x in sentences if len(x) != 0]\n",
    "    pre_covid_sentences.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "dG0MORGI7pNg"
   },
   "outputs": [],
   "source": [
    "pandemic_sentences = []\n",
    "link_headers = ['http://t.co/','https://t.co/']\n",
    "for tweet in pandemic_tweets:\n",
    "    tweets_wo_link = []\n",
    "    x = 0\n",
    "    y = 0    \n",
    "    if link_headers[0] in tweet:\n",
    "        x = 1\n",
    "    if link_headers[1] in tweet:\n",
    "        y = 1\n",
    "    while tweet:\n",
    "        if x == 0 and y == 0:\n",
    "            tweets_wo_link.append(tweet)\n",
    "            tweet = False\n",
    "        elif x == 0 and y == 1:\n",
    "            ind = tweet.find(link_headers[1]) \n",
    "            tweets_wo_link.append(tweet[0:ind])\n",
    "            tweet = tweet[ind:]\n",
    "            tweet = no_link(tweet)\n",
    "                \n",
    "            if link_headers[1] not in tweet:\n",
    "                y = 0\n",
    "                \n",
    "        elif x == 1 and y == 0:\n",
    "            ind = tweet.find(link_headers[0])\n",
    "            tweets_wo_link.append(tweet[0:ind])\n",
    "            tweet = tweet[ind:]\n",
    "            tweet = no_link(tweet)\n",
    "            if link_headers[0] not in tweet:\n",
    "                x = 0\n",
    "               \n",
    "        elif x == 1 and y == 1:\n",
    "            ind0 = tweet.find(link_headers[0])\n",
    "            ind1 = tweet.find(link_headers[1])\n",
    "            if ind0 < ind1:\n",
    "                ind = tweet.find(link_headers[0])\n",
    "                tweets_wo_link.append(tweet[0:ind])\n",
    "                tweet = tweet[ind:]\n",
    "                tweet = no_link(tweet)\n",
    "            elif ind0 > ind1:\n",
    "                ind = tweet.find(link_headers[1])\n",
    "                tweets_wo_link.append(tweet[0:ind])\n",
    "                tweet = no_link(tweet)\n",
    "            ind0 = tweet.find(link_headers[0])\n",
    "            ind1 = tweet.find(link_headers[1])\n",
    "            if link_headers[1] not in tweet:\n",
    "                y = 0\n",
    "            if link_headers[0] not in tweet:\n",
    "                x = 0\n",
    "    tweets_wo_link = [x for x in tweets_wo_link if len(x) != 0]\n",
    "    sentences = []\n",
    "    j = 0\n",
    "    for tweet_wo_link in tweets_wo_link:\n",
    "        for i, char in enumerate(tweet_wo_link):\n",
    "            if char in UNICODE_EMOJI['en'] or char in separators:\n",
    "                sentences.append(tweet_wo_link[j:i])\n",
    "                j = i + 1\n",
    "        sentences.append(tweet_wo_link[j:])\n",
    "    sentences = [x for x in sentences if len(x) != 0]\n",
    "    pandemic_sentences.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xsf3VR97pNh",
    "outputId": "ce83caa4-7ab3-49e7-9f6a-559baa973fac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pre_covid_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yt4CjyeF7pNh",
    "outputId": "2568ae23-fc89-4863-96e6-9b3302e748d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pandemic_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcPEs0Il7pNh"
   },
   "outputs": [],
   "source": [
    "pre_covid_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-nWDzdd7pNh"
   },
   "source": [
    "# Morphological Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0FbttrL7pNi"
   },
   "source": [
    "From Emre Can Açıkgöz's private code, we tag the tweets and get the lists 'pre_covid_tagged' and 'pandemic_tagged.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roPSQim77pNi"
   },
   "outputs": [],
   "source": [
    "pre_covid_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuXx2un07pNj"
   },
   "outputs": [],
   "source": [
    "pandemic_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4iLghU17pNj"
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "j4lbr7vk7pNj"
   },
   "outputs": [],
   "source": [
    "no_features = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "xM6cbMr97pNk"
   },
   "outputs": [],
   "source": [
    "pre_covid_features = np.zeros((len(pre_covid_tweets), no_features))\n",
    "pandemic_features = np.zeros((len(pandemic_tweets), no_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mArFflgs7pNk"
   },
   "source": [
    "Extract Feature 1: All Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "iunijyO87pNk"
   },
   "outputs": [],
   "source": [
    "punctuations = ['.', ',', ':', ';']\n",
    "\n",
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    x = 0\n",
    "    for punctuation in punctuations:\n",
    "        x += tweet.count(punctuation)\n",
    "    pre_covid_features[i, 0] = x\n",
    "\n",
    "for i, tweet in enumerate(pandemic_tweets):\n",
    "    x = 0\n",
    "    for punctuation in punctuations:\n",
    "        x += tweet.count(punctuation)\n",
    "    pandemic_features[i, 0] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JC_0fRYm7pNk"
   },
   "source": [
    "Extract Feature 2: Number of Commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "sghlocEw7pNk"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    pre_covid_features[i, 1] = tweet.count(',')\n",
    "\n",
    "for i, tweet in enumerate(pandemic_tweets):\n",
    "    pandemic_features[i, 1] = tweet.count(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyBiTlz_7pNk"
   },
   "source": [
    "Extract Feature 3: Number of '@' patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "YnIAFhBZ7pNk"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    pre_covid_features[i, 2] = len(re.findall(r\"@\\S+\", tweet))\n",
    "\n",
    "for i, tweet in enumerate(pandemic_tweets):\n",
    "    pandemic_features[i, 2] = len(re.findall(r\"@\\S+\", tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhFBV24b7pNl"
   },
   "source": [
    "Extract Feature 4: Number of '!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Fb_uc7dK7pNl"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    pre_covid_features[i, 3] = tweet.count('!')\n",
    "\n",
    "for i, tweet in enumerate(pandemic_tweets):\n",
    "    pandemic_features[i, 3] = tweet.count('!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1hwwYux7pNl"
   },
   "source": [
    "Extract Feature 5: Number of External Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "vL3BM86g7pNl"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    pre_covid_features[i, 4] = tweet.count('http://t.co/') + tweet.count('https://t.co/')\n",
    "\n",
    "for i, tweet in enumerate(pandemic_tweets):\n",
    "    pandemic_features[i, 4] = tweet.count('http://t.co/') + tweet.count('https://t.co/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmrBin167pNl"
   },
   "source": [
    "Extract Feature 6: Number of first person singular pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Ldrd4Yvi7pNl"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tagged):\n",
    "    for sentence in tweet:\n",
    "        for word in sentence:\n",
    "            if 'A1sg' in word:\n",
    "                pre_covid_features[i, 5] += 1\n",
    "for i, tweet in enumerate(pandemic_tagged):\n",
    "    for sentence in tweet:\n",
    "        for word in sentence:\n",
    "            if 'A1sg' in word:\n",
    "                pandemic_features[i, 5] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zpJReBE7pNm"
   },
   "source": [
    "Extract Feature 7 Using the Morphological Analyzer: the count of negative particles (değil, -me and -ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "jLHB9FAJ7pNm"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tagged):\n",
    "    for sentence in tweet:\n",
    "        for word in sentence:\n",
    "            if 'Neg' in word:\n",
    "                pre_covid_features[i, 6] += 1\n",
    "for i, tweet in enumerate(pandemic_tagged):\n",
    "    for sentence in tweet:\n",
    "        for word in sentence:\n",
    "            if 'Neg' in word:\n",
    "                pandemic_features[i, 6] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KhGVzIj7pNm"
   },
   "source": [
    "Extract Feture 8: Number of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "9dxp8JZ77pNm"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    pre_covid_features[i, 7] = sum(c.isdigit() for c in tweet)\n",
    "\n",
    "for i, tweet in enumerate(pandemic_tweets):\n",
    "    pandemic_features[i, 7] = sum(c.isdigit() for c in tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9W_Q7_l7pNm"
   },
   "source": [
    "Extract Feature 9: Number of parenthetical phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "OhBOZOzu7pNm"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    pre_covid_features[i, 8] = len(re.findall(r'\\(.+?\\)', tweet))\n",
    "\n",
    "for i, tweet in enumerate(pandemic_tweets):\n",
    "    pandemic_features[i, 8] = len(re.findall(r'\\(.+?\\)', tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgLZF6KU7pNm"
   },
   "source": [
    "Extract Feature 10: Number of prepositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "K8K-baFc7pNn"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tagged):\n",
    "    for sentence in tweet:\n",
    "        for word in sentence:\n",
    "            if 'Postp' in word:\n",
    "                pre_covid_features[i, 9] += 1\n",
    "for i, tweet in enumerate(pandemic_tagged):\n",
    "    for sentence in tweet:\n",
    "        for word in sentence:\n",
    "            if 'Postp' in word:\n",
    "                pandemic_features[i, 9] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfXkm-_U7pNn"
   },
   "source": [
    "Extract Feature 11: Number of pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "f_E-C2FS7pNn"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tagged):\n",
    "    for sentence in tweet:\n",
    "        for word in sentence:\n",
    "            if 'Pers' in word:\n",
    "                pre_covid_features[i, 10] += 1\n",
    "            if 'Pron' in word:\n",
    "                pre_covid_features[i, 10] += 1\n",
    "            if 'Pnon' in word:\n",
    "                pre_covid_features[i, 10] += 1\n",
    "            if 'Reflex' in word:\n",
    "                pre_covid_features[i, 10] += 1\n",
    "for i, tweet in enumerate(pandemic_tagged):\n",
    "    for sentence in tweet:\n",
    "        for word in sentence:\n",
    "            if 'Pers' in word:\n",
    "                pandemic_features[i, 10] += 1\n",
    "            if 'Pron' in word:\n",
    "                pandemic_features[i, 10] += 1\n",
    "            if 'Pnon' in word:\n",
    "                pandemic_features[i, 10] += 1\n",
    "            if 'Reflex' in word:\n",
    "                pandemic_features[i, 10] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXVx9cAr7pNn"
   },
   "source": [
    "Extract Feature 12: Number of '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "5Ha1wQI97pNn"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    pre_covid_features[i, 11] = tweet.count('?')\n",
    "    \n",
    "for i, tweet in enumerate(pandemic_tweets):\n",
    "    pandemic_features[i, 11] = tweet.count('?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJppbMVv7pNn"
   },
   "source": [
    "Extract Feature 13: Number of words longer than 6 letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "nd8jYHCq7pNn"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    x = 0\n",
    "    lengths = [len(word) for word in tweet.split()]\n",
    "    for length in lengths:\n",
    "        if length > 6:\n",
    "            x += 1\n",
    "    pre_covid_features[i, 12] = x\n",
    "    \n",
    "for i, tweet in enumerate(pandemic_tweets):\n",
    "    x = 0\n",
    "    lengths = [len(word) for word in tweet.split()]\n",
    "    for length in lengths:\n",
    "        if length > 6:\n",
    "            x += 1\n",
    "    pandemic_features[i, 12] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeDY3_0R7pNo"
   },
   "source": [
    "Extract Feature 14: Number of first person (singular and plural) pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "UEm7ePJR7pNo"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tagged):\n",
    "    for sentence in tweet:\n",
    "        for word in sentence:\n",
    "            if 'A1sg' in word:\n",
    "                pre_covid_features[i, 13] += 1\n",
    "            if 'P1sg' in word:\n",
    "                pre_covid_features[i, 13] += 1\n",
    "            if 'A1pl' in word:\n",
    "                pre_covid_features[i, 13] += 1\n",
    "            if 'P1pl' in word:\n",
    "                pre_covid_features[i, 13] += 1\n",
    "for i, tweet in enumerate(pandemic_tagged):\n",
    "    for sentence in tweet:\n",
    "        for word in sentence:\n",
    "            if 'A1sg' in word:\n",
    "                pandemic_features[i, 13] += 1\n",
    "            if 'P1sg' in word:\n",
    "                pandemic_features[i, 13] += 1\n",
    "            if 'A1pl' in word:\n",
    "                pandemic_features[i, 13] += 1\n",
    "            if 'P1pl' in word:\n",
    "                pandemic_features[i, 13] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zkgr96I7pNo"
   },
   "source": [
    "Extract Feature 15: Type/token ratio (tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "MYa7qq987pNo"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    words = tweet.lower().split()\n",
    "    tt = len(set(words)) / len(words)\n",
    "    pre_covid_features[i, 14] = tt\n",
    "    \n",
    "for i, tweet in enumerate(pandemic_tweets):\n",
    "    words = tweet.lower().split()\n",
    "    tt = len(set(words)) / len(words)\n",
    "    pandemic_features[i, 14] = tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFGOkKQ07pNo"
   },
   "source": [
    "Extract Feature 16: Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "uiZNxOBN7pNo"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    pre_covid_features[i, 15] = len(tweet.split())\n",
    "\n",
    "for i, tweet in enumerate(pandemic_tweets):\n",
    "    pandemic_features[i, 15] = len(tweet.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WIf2PUO7pNo"
   },
   "source": [
    "Extract Feature 17: Number of first person plural pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "6iJPExuB7pNo"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tagged):\n",
    "    for sentence in tweet:\n",
    "        for word in sentence:\n",
    "            if 'A1pl' in word:\n",
    "                pre_covid_features[i, 16] += 1\n",
    "            if 'P1pl' in word:\n",
    "                pre_covid_features[i, 16] += 1\n",
    "for i, tweet in enumerate(pandemic_tagged):\n",
    "    for sentence in tweet:\n",
    "        for word in sentence:\n",
    "            if 'A1pl' in word:\n",
    "                pandemic_features[i, 16] += 1\n",
    "            if 'P1pl' in word:\n",
    "                pandemic_features[i, 16] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_-3NehS7pNp"
   },
   "outputs": [],
   "source": [
    "\"\"\"fpps = [\"Biz\", \"Biz\", \"bız\", \"biz\"]\n",
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    x = 0\n",
    "    for fpp in fpps:\n",
    "        x += tweet.count(fpp)\n",
    "    pre_covid_features[i, 16] = x\n",
    "\n",
    "for i, tweet in enumerate(pandemic_tweets):\n",
    "    x = 0\n",
    "    for fpp in fpps:\n",
    "        x += tweet.count(fpp)\n",
    "    pandemic_features[i, 16] = x\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRpPKmkd7pNp"
   },
   "source": [
    "Extract Feature 18: Number of second person singular pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "99TYkXNf7pNp"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tagged):\n",
    "    for sentence in tweet:\n",
    "        for word in sentence:\n",
    "            if 'A2sg' in word:\n",
    "                pre_covid_features[i, 17] += 1\n",
    "            if 'P2sg' in word:\n",
    "                pre_covid_features[i, 17] += 1\n",
    "for i, tweet in enumerate(pandemic_tagged):\n",
    "    for sentence in tweet:\n",
    "        for word in sentence:\n",
    "            if 'A2sg' in word:\n",
    "                pandemic_features[i, 17] += 1\n",
    "            if 'P2sg' in word:\n",
    "                pandemic_features[i, 17] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVint1757pNp"
   },
   "outputs": [],
   "source": [
    "\"\"\"spss = [\"Sen\", \"sen\"]\n",
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    x = 0\n",
    "    for sps in spss:\n",
    "        x += tweet.count(sps)\n",
    "    pre_covid_features[i, 17] = x\n",
    "\n",
    "for i, tweet in enumerate(pandemic_tweets):\n",
    "    x = 0\n",
    "    for sps in spss:\n",
    "        x += tweet.count(sps)\n",
    "    pandemic_features[i, 17] = x\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBaJ3YJJ7pNp"
   },
   "source": [
    "Extract Feature 19: Simple mean of the frequency (mf) of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "3i6UvS2N7pNp"
   },
   "outputs": [],
   "source": [
    "pre_covid_vocabulary = {}\n",
    "for tweet in pre_covid_tweets:\n",
    "    words = tweet.lower().split()\n",
    "    for word in words:\n",
    "        pre_covid_vocabulary[word] = pre_covid_vocabulary.get(word, 0) + 1\n",
    "        \n",
    "pandemic_vocabulary = {}\n",
    "for tweet in pandemic_tweets:\n",
    "    words = tweet.lower().split()\n",
    "    for word in words:\n",
    "        pandemic_vocabulary[word] = pandemic_vocabulary.get(word, 0) + 1\n",
    "        \n",
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    words = tweet.lower().split()\n",
    "    sum_of_frequencies = 0.0\n",
    "    for word in words:\n",
    "        sum_of_frequencies += pre_covid_vocabulary.get(word, 1)\n",
    "    mf = sum_of_frequencies / len(words)\n",
    "    pre_covid_features[i, 18] = mf\n",
    "for i, tweet in enumerate(pandemic_tweets):\n",
    "    words = tweet.lower().split()\n",
    "    sum_of_frequencies = 0.0\n",
    "    for word in words:\n",
    "        sum_of_frequencies += pandemic_vocabulary.get(word, 1)\n",
    "    mf = sum_of_frequencies / len(words)\n",
    "    pandemic_features[i, 18] = mf   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkTwMjeJ7pNp"
   },
   "source": [
    "Feature 20: Number of words mentioned from the \"Extraverted Words\" list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n53irzjK7pNq",
    "outputId": "8afd648e-53b4-4240-c615-b79ac47b62d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CBwCSt67pNq",
    "outputId": "4fa02845-7298-4038-c564-b6810aeef71a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beraber',\n",
       " 'birlikte',\n",
       " 'ekip',\n",
       " 'takım',\n",
       " 'dost',\n",
       " 'sosyal',\n",
       " 'arkadaş',\n",
       " 'buluş',\n",
       " 'paylaş',\n",
       " 'bir arada',\n",
       " 'bir araya']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraverted_words = []\n",
    "with open('extraverted_words.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        extraverted_words.append(line.rstrip())\n",
    "extraverted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "BDxGPj4W7pNq"
   },
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(pre_covid_tweets):\n",
    "    for word in extraverted_words:\n",
    "        if word in tweet:\n",
    "            pre_covid_features[(i,19)] += 1\n",
    "\n",
    "for i, tweet in enumerate(pandemic_tweets):\n",
    "    for word in extraverted_words:\n",
    "        if word in tweet:\n",
    "            pandemic_features[(i,19)] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FOcDKZx7pNq"
   },
   "source": [
    "# Calculate the Means and Standard Deviations of the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "kGP7l7Gt7pNq"
   },
   "outputs": [],
   "source": [
    "mean_pre_covid = np.mean(pre_covid_features, axis=0)\n",
    "mean_pandemic = np.mean(pandemic_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "mdXsvnXH7pNq"
   },
   "outputs": [],
   "source": [
    "std_pre_covid = np.std(pre_covid_features, axis=0)\n",
    "std_pandemic = np.std(pandemic_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "YC5ddbm-7pNq"
   },
   "outputs": [],
   "source": [
    "threshold_pre_covid = mean_pre_covid + std_pre_covid\n",
    "threshold_pandemic = mean_pandemic + std_pandemic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUtjtTw77pNq"
   },
   "source": [
    "# Compute the Extraversion Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "KqrSpMittyz8"
   },
   "outputs": [],
   "source": [
    "neg_correlation_indices = []\n",
    "pos_correlation_indices = [19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "lOxcCbC97pNr"
   },
   "outputs": [],
   "source": [
    "pos_correlation_pre = pre_covid_features[:, pos_correlation_indices]\n",
    "neg_correlation_pre = pre_covid_features[:, neg_correlation_indices]\n",
    "pos_correlation_pandemic = pandemic_features[:, pos_correlation_indices]\n",
    "neg_correlation_pandemic = pandemic_features[:, neg_correlation_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "tQWml1wZ7pNr"
   },
   "outputs": [],
   "source": [
    "pos_scores_pre = np.sum(pos_correlation_pre >= threshold_pre_covid[pos_correlation_indices], axis=1)\n",
    "neg_scores_pre = np.sum(neg_correlation_pre >= threshold_pre_covid[neg_correlation_indices],axis=1)\n",
    "pos_scores_pandemic = np.sum(pos_correlation_pandemic >= threshold_pandemic[pos_correlation_indices], axis=1)\n",
    "neg_scores_pandemic = np.sum(neg_correlation_pandemic >= threshold_pandemic[neg_correlation_indices],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "X9hfyySe7pNr"
   },
   "outputs": [],
   "source": [
    "scores_pre = pos_scores_pre - neg_scores_pre\n",
    "scores_pandemic = pos_scores_pandemic - neg_scores_pandemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "9mjyW4jA7pNr"
   },
   "outputs": [],
   "source": [
    "neg_correlation_indices_high = [0, 2, 6, 8, 11, 12, 14]\n",
    "neg_correlation_indices_moderate = [4]\n",
    "neg_correlation_indices_weak = [1, 7, 15, 17]\n",
    "pos_correlation_indices_high = [10, 13, 16, 19]\n",
    "pos_correlation_indices_moderate = [5, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "o9L8TKaax7Sb"
   },
   "outputs": [],
   "source": [
    "pos_scores_pre_h = np.sum(pre_covid_features[:, pos_correlation_indices_high] >= threshold_pre_covid[pos_correlation_indices_high], axis=1)\n",
    "pos_scores_pre_m = np.sum(pre_covid_features[:, pos_correlation_indices_moderate] >= threshold_pre_covid[pos_correlation_indices_moderate], axis=1)\n",
    "neg_scores_pre_h = np.sum(pre_covid_features[:, neg_correlation_indices_high] >= threshold_pre_covid[neg_correlation_indices_high], axis=1)\n",
    "neg_scores_pre_m = np.sum(pre_covid_features[:, neg_correlation_indices_moderate] >= threshold_pre_covid[neg_correlation_indices_moderate], axis=1)\n",
    "neg_scores_pre_w = np.sum(pre_covid_features[:, neg_correlation_indices_weak] >= threshold_pre_covid[neg_correlation_indices_weak], axis=1)\n",
    "pos_scores_pandemic_h = np.sum(pandemic_features[:, pos_correlation_indices_high] >= threshold_pandemic[pos_correlation_indices_high], axis=1)\n",
    "pos_scores_pandemic_m = np.sum(pandemic_features[:, pos_correlation_indices_moderate] >= threshold_pandemic[pos_correlation_indices_moderate], axis=1)\n",
    "neg_scores_pandemic_h = np.sum(pandemic_features[:, neg_correlation_indices_high] >= threshold_pandemic[neg_correlation_indices_high], axis=1)\n",
    "neg_scores_pandemic_m = np.sum(pandemic_features[:, neg_correlation_indices_moderate] >= threshold_pandemic[neg_correlation_indices_moderate], axis=1)\n",
    "neg_scores_pandemic_w = np.sum(pandemic_features[:, neg_correlation_indices_weak] >= threshold_pandemic[neg_correlation_indices_weak], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "z-MUc7N2zJ-Z"
   },
   "outputs": [],
   "source": [
    "scores_pre = 5*pos_scores_pre_h + 4*pos_scores_pre_m - 5*neg_scores_pre_h -4*neg_scores_pre_m - 3*neg_scores_pre_w\n",
    "scores_pandemic = 5*pos_scores_pandemic_h +4*pos_scores_pandemic_m - 5*neg_scores_pandemic_h - 4*neg_scores_pandemic_m -3*neg_scores_pandemic_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Km1C4gc97pNr",
    "outputId": "6ef91207-c3ca-45c6-e5a2-e55a34ab62ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Oh4URpj7pNr",
    "outputId": "23bf97f2-ba57-47cd-e069-03668dee3dca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_pandemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4wwGTNY7pNr",
    "outputId": "c28c4cdf-1f86-419b-9ce2-439600cc707b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average extraversion score for tweets shared before the pandemic:  0.048\n",
      "Average extraversion score for tweets shared during the pandemic:  0.06\n"
     ]
    }
   ],
   "source": [
    "pre_covid_extraversion = np.mean(scores_pre)\n",
    "pandemic_extraversion = np.mean(scores_pandemic)\n",
    "print(\"Average extraversion score for tweets shared before the pandemic: \", pre_covid_extraversion)\n",
    "print(\"Average extraversion score for tweets shared during the pandemic: \", pandemic_extraversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "KMWcrym-7pNs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "1-project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
